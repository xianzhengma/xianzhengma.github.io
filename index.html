
<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <title>Xianzheng Ma</title>
    <link href="assets/images/logo.png" rel="shortcut icon">
</head>

<!-- Stylesheets -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/css/bootstrap.min.css" integrity="sha512-P5MgMn1jBN01asBgU0z60Qk4QxiXo86+wlFahKrsQf37c9cro517WzVSPPV1tDKzhku2iJ2FVgL67wG03SGnNA==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.1/css/all.min.css" integrity="sha512-+4zCK9k+qNFUR5X+cKL9EIR+ZOhtIloNl9GIKS57V1MyNsYpYcUrUeQc9vNfzsWfV28IaLL3i96P9sdNyeRssA==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.1/css/academicons.min.css" integrity="sha512-b1ASx0WHgVFL5ZQhTgiPWX+68KjS38Jk87jg7pe+qC7q9YkEtFq0z7xCglv7qGIs/68d3mAp+StfC8WKC5SSAg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css">
<link rel="stylesheet" href="assets/css/global.css">


<body class="bg-light" >
    <nav class="navbar navbar-expand-lg navbar-light bg-light fixed-top mb-5 shadow-sm">
    <div class="container">
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>

        <div class="collapse navbar-collapse" id="navbarResponsive">
            <ul class="navbar-nav ml-auto">
                
                <li class="nav-item active">
                    <a class="nav-link" href="/">Home
                        <span class="sr-only">(current)</span>
                    </a>
                </li>
                <!-- <li class="nav-item ">
                    <a class="nav-link" href="misc.html">Misc
                        
                    </a>
                </li> -->
                <!-- <li class="nav-item ">
                    <a class="nav-link" href="/resources">Resources -->
                        
                    </a>
                </li>
                
            </ul>
        </div>
    </div>
</nav>
    <div class="container">
        <div class="card border-0 shadow-sm bg-white">
    <div class="card-body p-5">
        <div class="row">
            <div class="col">
                <p class="h2 font-weight-normal">Xianzheng Ma <span>(é©¬å®ªæ”¿)</span></h1>
                <p class="text-muted">
                    I am currently a DPhil Student (10.2023- ) at Department of Engineering Science, University of Oxford, supervised by <a  target="_blank" href="https://www.robots.ox.ac.uk/~victor//">Prof. Victor Prisacariu</a>. 
                    Previously, I was a full-time researcher at Shanghai AI Lab, under the supervision of <a  target="_blank" href="http://xpixel.group/2010/01/20/chaodong.html">Prof. Chao Dong</a>. I obtained my Bachelor's and Master's degrees from Wuhan University in 2018 and 2021. <br> <br> 
                    My research interests include LLMs, 3D computer vision and robotics, especially using LLMs's world knowledge to enpower the 3D world understanding and interaction. <br> <br> 
                    For any suggestions or collaborations, please reach out to me at xianzheng@robots.ox.ac.uk.
                </p>
                <p class="text-muted">

                </p>
            </div>
            <div class="col-md-auto">
                <figure class="figure">
                    <img 
                        src="assets/images/portrait_L.jpg" 
                        class="figure-img img-fluid img-thumbnail" 
                        style="height: 180px;"
                        data-toggle="tooltip" 
                        data-placement="top" 
                    >
                    <figcaption class="figure-caption text-right"></figcaption>
                </figure>
            </div>

        </div>
        <div class="row h3">
            <div class="col">
                <a class="pr-3" href="mailto:xianzheng@robots.ox.ac.uk">
                    <i class="fa fa-envelope"></i></a>
                <a class="pr-3" target="_blank" href="https://github.com/xianzhengma">
                    <i class="fab fa-github"></i></a>
                <a class="pr-3" target="_blank" href="https://scholar.google.com/citations?user=NS8g2mMAAAAJ&hl=zh-CN&oi=ao">
                    <i class="fab fa-google"></i></a>
                <a class="pr-3" target="_blank" href="https://www.linkedin.com/in/xianzheng-ma-34035126b/">
                    <i class="fab fa-linkedin"></i></a>
            </div>
        </div>
    </div>
</div>

<div class="my-3 p-0 bg-white shadow-sm rounded-sm">
    <h4 class="border-bottom border-gray p-3 mb-0">
        <i class="fas"></i> News & Updates
   </h4>
   <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0 text-muted">
    <ul>
    <li class="text-muted">
        [2024-05-16]&nbsp; ðŸ“¢ Check out our <a href="https://arxiv.org/pdf/2405.10255">survey paper</a>s for 3D-related tasks empowered by LLMs and other foundataion models.
    </li>
    <li class="text-muted">
        [2023-12-16]&nbsp; We curated a <a href="https://github.com/ActiveVisionLab/Awesome-LLM-3D">Awesome-LLM-3D paper list</a> for 3D-related tasks empowered by LLMs.
    </li>
    <li class="text-muted">
        [2023-12-15]&nbsp; Two papers are accepted in AAAI2024.
    </li>
   <li class="text-muted">
        [2023-10-02]&nbsp; Start a new journey as DPhil (PhD) at <a href="https://www.robots.ox.ac.uk/ActiveVision/index.html">Active Vision Lab</a> in University of Oxford!
    </li>
</ul>
    </div>
</div>


<div class="my-3 p-0 bg-white shadow-sm rounded-sm">
    <h4 class="border-bottom border-gray p-3 mb-0">
        <i class="fas"></i> Selected Researches 
   </h4>


   <div class="row no-gutters position-relative border-bottom border-gray">
    <div class="col-md-3 col-xl-2 mb-md-0 p-md-3">
        <img src="/assets/pub/2024/survey/Figure_1_v5.png" class="lazy w-100 rounded-sm">
    </div>
    <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0">
        <h5 class="mt-0 mb-1 font-weight-normal">When LLMs step into the 3D World: A Survey and Meta-Analysis of 3D Tasks via Multi-modal Large Language Models
</h5>
        <p class="mt-0 mb-0 small">
        <span class="comma text-body"><strong>Xianzheng Ma</strong></span>
        <a class="comma text-body" target="_blank">Yash Bhalgat</a>
        <span class="comma text-body" target="_blank">Brandon Smart</span>
        <span class="comma text-body" target="_blank">Shuai Chen</span>
        <span class="comma text-body" target="_blank">Xinghui Li</span>
        <span class="comma text-body" target="_blank">Jian Ding</span>
        <span class="comma text-body" target="_blank">Jindong Gu</span>
        <span class="comma text-body" target="_blank">Dave Zhenyu Chen</span>
        <span class="comma text-body" target="_blank">Songyou Peng</span>
        <span class="comma text-body" target="_blank">Jia-Wang Bian</span>
        <span class="comma text-body" target="_blank">Philip H Torr</span>
        <span class="comma text-body" target="_blank">Marc Pollefeys</span>
        <span class="comma text-body" target="_blank">Matthias NieÃŸner</span>
        <span class="comma text-body" target="_blank">Ian D Reid</span>
        <span class="comma text-body" target="_blank">Angel X. Chang</span>
        <span class="comma text-body" target="_blank">Iro Laina</span>
        <span class="comma text-body" target="_blank">Victor Adrian Prisacariu</span>
</p>

        <p class="mt-0 mb-0 small"><i>TPAMI under review
</i> 
        <p class="mt-0 mb-0 small text-muted"></p>
        
        <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
            
            
            <a target="_blank" href="https://arxiv.org/pdf/2405.10255">paper</a> |
            <a target="_blank" href="https://arxiv.org/pdf/2405.10255">arxiv</a> | 
            <a target="_blank" href="">project page</a> |
            <a target="_blank" href="https://github.com/ActiveVisionLab/Awesome-LLM-3D">code</a> |
            <a target="_blank" href="">bibtex</a> 
            
        </p>


        <p class="mt-0 mb-0 small text-muted">We survey the papers of 3D understanding, generation, and embodied agent tasks empowered by LLMs and other foundataion models (CLIP, SAM) </p>

    </div>
</div>  
   
   <div class="row no-gutters position-relative border-bottom border-gray">
    <div class="col-md-3 col-xl-2 mb-md-0 p-md-3">
        <img src="/assets/pub/2023/match/point-bind.jpg" class="lazy w-100 rounded-sm">
    </div>
    <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0">
        <h5 class="mt-0 mb-1 font-weight-normal">Point-Bind & Point-LLM: Aligning Point Cloud with Multi-modality for 3D Understanding, Generation, and Instruction Following
</h5>
        <p class="mt-0 mb-0 small">
        <a class="comma text-body" target="_blank">Ziyu Guo</a>
        <span class="comma text-body" target="_blank">Renrui Zhang</span>
        <span class="comma text-body" target="_blank">Xiangyang Zhu</span>
        <span class="comma text-body" target="_blank">Yiwen Tang</span>
        <span class="comma text-body"><strong>Xianzheng Ma</strong></span>
        <span class="comma text-body" target="_blank">Jiaming Han</span>
        <span class="comma text-body" target="_blank">Kexin Chen</span>
        <span class="comma text-body" target="_blank">Peng Gao</span>
        <span class="comma text-body" target="_blank">Xianzhi Li</span>
        <span class="comma text-body" target="_blank">Hongsheng Li</span>
        <span class="comma text-body" target="_blank">Pheng-Ann Heng</span>
</p>

        <p class="mt-0 mb-0 small"><i>ICLR under review
</i> 
        <p class="mt-0 mb-0 small text-muted"></p>
        
        <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
            
            
            <a target="_blank" href="https://arxiv.org/pdf/2309.00615.pdf">paper</a> |
            <a target="_blank" href="https://arxiv.org/pdf/2309.00615.pdf">arxiv</a> | 
            <a target="_blank" href="">project page</a> |
            <a target="_blank" href="https://github.com/ZrrSkywalker/Point-Bind_Point-LLM">code</a> |
            <a target="_blank" href="">bibtex</a> 
            
        </p>


        <p class="mt-0 mb-0 small text-muted">We propose a 3D multi-modal model for general 3D learning, Point-Bind, and the first 3D large language model, Point-LLM </p>

    </div>
</div>    

<div class="row no-gutters position-relative border-bottom border-gray">
    <div class="col-md-3 col-xl-2 mb-md-0 p-md-3">
        <img src="/assets/pub/2023/match/match.png" class="lazy w-100 rounded-sm">
    </div>
    <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0">
        <h5 class="mt-0 mb-1 font-weight-normal">Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation
</h5>
        <p class="mt-0 mb-0 small">
        <a class="comma text-body" target="_blank">Min Shi</a>
        <span class="comma text-body" target="_blank">Zihao Huang</span>
        <span class="comma text-body"><strong>Xianzheng Ma</strong></span>
        <span class="comma text-body" target="_blank">Xiaowei Hu</span>
        <span class="comma text-body" target="_blank">Zhiguo Cao</span>
</p>
        <p class="mt-0 mb-0 small"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
</i> 2023 <span class="badge badge-pill badge-custom badge-success">Highlight</span></p>
        <p class="mt-0 mb-0 small text-muted"></p>
        
        <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
            
            
            <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2023_paper.pdf">paper</a> |
            <a target="_blank" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2023_paper.pdf">arxiv</a> | 
            <a target="_blank" href="">project page</a> |
            <a target="_blank" href="github.com/flyinglynx/CapeFormer">code</a> |
            <a target="_blank" href="">bibtex</a> 
            
        </p>


        <p class="mt-0 mb-0 small text-muted">We propose a two-stage framework--CapeFormer for category-agnostic pose estimation. </p>

    </div>
</div>    

<div class="row no-gutters position-relative border-bottom border-gray">
    <div class="col-md-3 col-xl-2 mb-md-0 p-md-3">
        <img src="assets/pub/2022/rsgr/thumbnail1.png" class="lazy w-100 rounded-sm">
    </div>
    <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0">
        <h5 class="mt-0 mb-1 font-weight-normal">Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation
</h5>
        <p class="mt-0 mb-0 small">
        
        <span class="comma text-body" target="_blank">Yulu Gan</span>
        <span class="comma text-body"><strong>Xianzheng Ma</strong></span>
        <span class="comma text-body" target="_blank">Yan Bai</span>
        <span class="comma text-body" target="_blank">Yihang Lou</span>
        <span class="comma text-body" target="_blank">Renrui Zhang</span>
        <span class="comma text-body" target="_blank">Nian Shi</span>
        <span class="comma text-body" target="_blank">Lin Luo</span>
</p>
        <p class="mt-0 mb-0 small"><i> AAAI Conference on Artificial Intelligence (AAAI)
</i> 2023 <span class="badge badge-pill badge-custom badge-success">Outstanding Student Paper</span></p>
        <p class="mt-0 mb-0 small text-muted"></p>
        
        <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
            
            
            <a target="_blank" href="https://arxiv.org/pdf/2212.04145.pdf">paper</a> | 
            <a target="_blank" href="https://arxiv.org/pdf/2212.04145.pdf">arxiv</a> |
            <a target="_blank" href="">project page</a> |
            <a target="_blank" href="">code</a> |
            <a target="_blank" href="">bibtex</a>
            
            
        </p>

        <p class="mt-0 mb-0 small text-muted">We offer an alternative and new solution for continual test-time adapation by learning visual domain prompt.</p>

    </div>
</div>  
       
<div class="row no-gutters position-relative border-bottom border-gray">
    <div class="col-md-3 col-xl-2 mb-md-0 p-md-3">
        <img src="assets/pub/2022/cdm/thumbnail.png" class="lazy w-100 rounded-sm">
    </div>
    <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0">
        <h5 class="mt-0 mb-1 font-weight-normal">Both Style and Fog Matter: Cumulative Domain Adaptation for Semantic Foggy Scene Understanding
</h5>
        <p class="mt-0 mb-0 small">
        <span class="comma text-body"><strong>Xianzheng Ma</strong></span>
        <a class="comma text-body" target="_blank" href="https://scholar.google.com/citations?user=NS8g2mMAAAAJ&hl=zh-CN">Zhixiang Wang</a>
        <span class="comma text-body" target="_blank">Yacheng Zhan</span>
        <a class="comma text-body" target="_blank" href="https://researchmap.jp/yinqiangzheng?lang=en">Yinqiang Zheng</a>
        <a class="comma text-body" target="_blank" href="https://wangzwhu.github.io/home/">Zheng Wang</a>
        <a class="comma text-body" target="_blank" href="https://vas.mpi-inf.mpg.de/dengxin/">Dengxin Dai</a>
        <a class="comma text-body" target="_blank" href="https://www.ee.nthu.edu.tw/cwlin/">Chia-Wen Lin</a>
</p>
        <p class="mt-0 mb-0 small"><i>IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)
</i> 2022 <span class="badge badge-pill badge-custom badge-success">Oral</span></p>
        <p class="mt-0 mb-0 small text-muted"></p>
        
        <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
            
            
            <a target="_blank" href="https://arxiv.org/abs/2112.00484">paper</a> |
            <a target="_blank" href="https://arxiv.org/abs/2112.00484">arxiv</a> | 
            <a target="_blank" href="">project page</a> |
            <a target="_blank" href="">code</a> |
            <a target="_blank" href="">bibtex</a> 
            
        </p>


        <p class="mt-0 mb-0 small text-muted">We alleviate the domain gap caused by mixed fog influence and style variation without labels.</p>

    </div>
</div>  
    
<div class="row no-gutters position-relative border-bottom border-gray">
    <div class="col-md-3 col-xl-2 mb-md-0 p-md-3">
        <img src="assets/pub/2022/rsgr/thumbnail2.png" class="lazy w-100 rounded-sm">
    </div>
    <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0">
        <h5 class="mt-0 mb-1 font-weight-normal">REMOTE: Reinforced Motion Transformation Network for Semi-supervised 2D Pose Estimation in Videos
</h5>
        <p class="mt-0 mb-0 small">
        <span class="comma text-body"><strong>Xianzheng Ma</strong></span>
        <span class="comma text-body" target="_blank">Hossein Rahmani</span>
        <span class="comma text-body" target="_blank">Zhipeng Fan</span>
        <span class="comma text-body" target="_blank">Bin Yang</span>
        <span class="comma text-body" target="_blank">Jun Chen</span>
        <span class="comma text-body" target="_blank">Jun Liu</span>
</p>
        <p class="mt-0 mb-0 small"><i> AAAI Conference on Artificial Intelligence (AAAI)
</i> 2022</p>
        <p class="mt-0 mb-0 small text-muted"></p>
        
        <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
            
            
            <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/20089/19848">paper</a> | 
            <a target="_blank" href="">bibtex</a>
            
            
        </p>

        <p class="mt-0 mb-0 small text-muted">We offer an reinforcement learning based method to ultilize the temporal information in videos to train a robust pose estimator.</p>

    </div>
</div>  
    
<div class="row no-gutters position-relative border-bottom border-gray">
    <div class="col-md-3 col-xl-2 mb-md-0 p-md-3">
        <img src="assets/pub/2020/dsm/thumbnail3.png" class="lazy w-100 rounded-sm">
    </div>
    <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0">
        <h5 class="mt-0 mb-1 font-weight-normal">Rainy WCity: A Real Rainfall Dataset with Diverse Conditions for Semantic Driving Scene Understanding
</h5>
        <p class="mt-0 mb-0 small">
        <span class="comma text-body" target="_blank">Xian Zhong*</span>
        <span class="comma text-body"><strong>Xianzheng Ma*</strong></span>
        <span class="comma text-body" target="_blank">Shidong Tu</span>
        <span class="comma text-body" target="_blank">Kui Jiang</span> 
        <span class="comma text-body" target="_blank">Wenxin Huang</span>
        <span class="comma text-body" target="_blank">Zheng Wang</span>
        <span class="comma text-body" target="_blank">(* means equal contribution)</span>
</p>
        <p class="mt-0 mb-0 small"><i>International Joint Conference on Artificial Intelligence (IJCAI)</i> 2022</p>
        <p class="small pb-0 mb-0 lh-125 text-muted abstract-links">
            
            <a target="_blank" href="https://www.ijcai.org/proceedings/2022/0243.pdf">paper</a> | 
            <a target="_blank" href="https://www.ijcai.org/proceedings/2022/0243.pdf">arxiv</a> | 
            <a target="_blank" href="">bibtex</a> 

            
        </p>

        <p class="mt-0 mb-0 small text-muted"></p>

        <p class="mt-0 mb-0 small text-muted">We propose a real-world rainy driving dataset for semantic segmentation and devise an unsupervised joint optimization framework based on contrastive learning. 
        
        </p>

    </div>
</div>

<h6 class="d-block p-3 mt-0 text-right">
    <a href="https://scholar.google.com/citations?user=NS8g2mMAAAAJ&hl=zh-CN&oi=ao">All publications <i class="fas fa-angle-double-right"></i></a>
</h6>


</div>

    <div class="my-3 p-0 bg-white shadow-sm rounded-sm">
        <h4 class="border-bottom border-gray p-3 mb-0">
            <i class="fas"></i> Academic Services
       </h4>
       <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0 text-muted">
        <ul>
        <li class="text-muted">
            <strong>Conference Reviewer:</strong> CVPR'2021-2023, AAAI'2022-2025, ICLR'2023, NeurIPS'2024.
        </li>
        <li class="text-muted">
            <strong>Journal Reviewer:</strong> IJCV, TMM, 
        </li>
        <li class="text-muted">
            <strong>Area Chair:</strong> IJCAI'2022 (China branch)
    </ul>
        </div>
    </div>

    <div class="my-3 p-0 bg-white shadow-sm rounded-sm">
        <h4 class="border-bottom border-gray p-3 mb-0">
            <i class="fas"></i> Misc.
       </h4>
       <div class="col-md-9 col-xl-10 position-static p-3 pl-md-0 text-muted">
        <ul>
        <li class="text-muted">
            <strong>Sports:</strong> Swimming, Badminton, Table Tennis, etc.
        </li>
    </ul>
        </div>
    </div>


    <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=1nUoTTQz0BXhp7T5dFnB1j9r2pX7sCHleOHRdYAkeII&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff'></script>



<footer class="footer">
    <div class="container">
        <div class="row my-3">
            <div class="col-6">
                <small class="text-muted"><i>Last updated: 2023-12-29</i></small>
            </div>
            <div class="col-6">
                <div class="float-right text-muted"><i>template from <a href="https://luost.me">luost26</a></i></div>
            </div>
        </div>
    </div>
</footer>



</body>

<!-- Scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.min.js"></script>
<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/jquery.lazy/1.7.9/jquery.lazy.plugins.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js" integrity="sha384-UO2eT0CpHqdSJQ6hJty5KVphtPhzWj9WO1clHTMGa3JDZwrnQq4sF86dIHNDz0W1" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/4.6.0/js/bootstrap.min.js" integrity="sha512-XKa9Hemdy1Ui3KSGgJdgMyYlUg1gM+QhL6cnlyTe2qzMCYm4nAZ1PsVerQzTTXzonUR+dmswHqgJPuwCq1MaAg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/github-buttons/2.14.2/buttons.min.js" integrity="sha512-OYwZx04hKFeFNYrWxIyo3atgGpb+cxU0ENWBZs72X7T9U+NoHPM1ftUn/Mfw7dRDXrqWA6M1wBg6z6fGE32aeA==" crossorigin="anonymous"></script>
<script src="assets/js/common.js"></script>


<script>
 (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
 (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
 m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
 })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

 ga('create', 'UA-104390830-1', 'auto');
 ga('send', 'pageview');

</script>


</html>
